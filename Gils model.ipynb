{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2236708,"sourceType":"datasetVersion","datasetId":1343913}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2  # for image processing\nfrom PIL import Image\nimport os\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.model_selection import train_test_split\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T20:59:17.591792Z","iopub.execute_input":"2024-11-02T20:59:17.592264Z","iopub.status.idle":"2024-11-02T20:59:34.618620Z","shell.execute_reply.started":"2024-11-02T20:59:17.592219Z","shell.execute_reply":"2024-11-02T20:59:34.617420Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"healthy_filepath = \"/kaggle/input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set/Healthy\"\nunhealthy_filepath = \"/kaggle/input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set/Brain Tumor\"\n\nhealthy_count = len(os.listdir(healthy_filepath))\nunhealthy_count = len(os.listdir(unhealthy_filepath))\n\nprint(f\"Healthy Images: {healthy_count}, Unhealthy Images: {unhealthy_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T20:59:34.620631Z","iopub.execute_input":"2024-11-02T20:59:34.621316Z","iopub.status.idle":"2024-11-02T20:59:35.169291Z","shell.execute_reply.started":"2024-11-02T20:59:34.621274Z","shell.execute_reply":"2024-11-02T20:59:35.168249Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Healthy Images: 2087, Unhealthy Images: 2513\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Images","metadata":{}},{"cell_type":"code","source":"# standardize all of the images\ndef load_images_from_folder(folder, label, image_size=(128, 128)):\n    images = []\n    labels = []\n    for filename in os.listdir(folder):\n        filepath = os.path.join(folder, filename)\n        try: \n            img = cv2.imread(filepath)\n            if img is not None: # if openCV can't read the image try with PIL\n                img = Image.open(filepath)\n                img = img.convert(\"RGB\") # convert to RGB in case of grayscale or RGBA\n                img = np.array(img)\n\n            img = cv2.resize(img, image_size)\n            images.append(img)\n            labels.append(label)\n        \n        except Exception as e:\n            print(f\"Error loading {filename}: {e}\")\n            continue  # Skip unreadable images\n            \n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-11-02T20:59:35.170760Z","iopub.execute_input":"2024-11-02T20:59:35.171154Z","iopub.status.idle":"2024-11-02T20:59:35.181493Z","shell.execute_reply.started":"2024-11-02T20:59:35.171085Z","shell.execute_reply":"2024-11-02T20:59:35.180219Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# create arrays for images and labels\nhealthy_images, healthy_labels = load_images_from_folder(healthy_filepath, label = 0)\nunhealthy_images, unhealthy_labels = load_images_from_folder(unhealthy_filepath, label = 1)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T20:59:35.184186Z","iopub.execute_input":"2024-11-02T20:59:35.184589Z","iopub.status.idle":"2024-11-02T21:00:32.659251Z","shell.execute_reply.started":"2024-11-02T20:59:35.184550Z","shell.execute_reply":"2024-11-02T21:00:32.658065Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"X = np.array(healthy_images + unhealthy_images) / 255.0\ny = np.array(healthy_labels + unhealthy_labels)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:32.661331Z","iopub.execute_input":"2024-11-02T21:00:32.662394Z","iopub.status.idle":"2024-11-02T21:00:33.562593Z","shell.execute_reply.started":"2024-11-02T21:00:32.662324Z","shell.execute_reply":"2024-11-02T21:00:33.561445Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**X explained:**\n    This helps normalize the pixel values from their original range of [0, 255] to [0,1]. Normalizing helps the model learn more efficiently and can lead to better training results since most neural networks work better with normalized data. \n    \n","metadata":{}},{"cell_type":"code","source":"# Data Augmentation\ndata_gen = ImageDataGenerator(\n    rotation_range=5,      # Smaller rotation range\n    zoom_range=0.1,        # Small zoom to avoid losing details\n    width_shift_range=0.05, # Small shifts to keep important features within the frame\n    height_shift_range=0.05,\n    fill_mode=\"nearest\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:33.563836Z","iopub.execute_input":"2024-11-02T21:00:33.564205Z","iopub.status.idle":"2024-11-02T21:00:33.570056Z","shell.execute_reply.started":"2024-11-02T21:00:33.564166Z","shell.execute_reply":"2024-11-02T21:00:33.568781Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Data Augmentation:** Is meant to help prevent overfitting. \n* Rotation Range: randomly rotates images up to 20 degress which helps the model gereralize to slightly rotated images.\n* Zoom Range: randomly zooms in or out on images by up to 15% which helps the model learn to recognize images at different scales.\n* Width Shift Range and Height Shift Range: Randomly shifts images horizontally and vertically by up to 20%. This helps the model generalize slightly to images that may not be perfectly centered.\n* Shear Range: Shear transformation means essentially \"slanting\" the image by up to 15% which helps robustness with slight distortions.\n* Fill Mode: Speicifies how to fill in new pixel values when shifting, zooming, or rotating an image. ","metadata":{}},{"cell_type":"markdown","source":"# Split Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Separate healthy and unhealthy data based on labels\nX_healthy = X[y == 0]\nX_unhealthy = X[y == 1]\n\n# Step 1: Training Set - 60% of total data, balanced 50/50 between healthy and unhealthy\ntrain_size_per_class = 540  # 540 healthy, 540 unhealthy\nX_train = np.concatenate((X_healthy[:train_size_per_class], X_unhealthy[:train_size_per_class]))\ny_train = np.array([0] * train_size_per_class + [1] * train_size_per_class)\n\n# Step 2: Validation Set - 30% of total data, with an 80/20 split\nval_size_healthy = 432  # 80% healthy\nval_size_unhealthy = 108  # 20% unhealthy\nX_val = np.concatenate((X_healthy[train_size_per_class:train_size_per_class + val_size_healthy], \n                        X_unhealthy[train_size_per_class:train_size_per_class + val_size_unhealthy]))\ny_val = np.array([0] * val_size_healthy + [1] * val_size_unhealthy)\n\n# Step 3: Testing Set - 10% of total data, with an 80/20 split\ntest_size_healthy = 144  # 80% healthy\ntest_size_unhealthy = 36  # 20% unhealthy\nX_test = np.concatenate((X_healthy[train_size_per_class + val_size_healthy:train_size_per_class + val_size_healthy + test_size_healthy], \n                         X_unhealthy[train_size_per_class + val_size_unhealthy:train_size_per_class + val_size_unhealthy + test_size_unhealthy]))\ny_test = np.array([0] * test_size_healthy + [1] * test_size_unhealthy)\n\n# Verification\nprint(\"Training set:\", len(X_train), len(y_train))  # Expected: 1080\nprint(\"Validation set:\", len(X_val), len(y_val))    # Expected: 540\nprint(\"Testing set:\", len(X_test), len(y_test))      # Expected: 180\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:33.572077Z","iopub.execute_input":"2024-11-02T21:00:33.572516Z","iopub.status.idle":"2024-11-02T21:00:34.591299Z","shell.execute_reply.started":"2024-11-02T21:00:33.572476Z","shell.execute_reply":"2024-11-02T21:00:34.589912Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training set: 1080 1080\nValidation set: 540 540\nTesting set: 180 180\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training Set (2760 Images):**\n* We select the first 1380 healthy and the first 1380 unhealthy images to ensure a 50/50 split.\n\n**Validation Set (1380 Images):**\n* We select the next 1104 healthy images and 276 unhealthy images to achieve an 80/20 split.\n\n**Testing Set (460 Images):**\n* Finally, we select the next 368 healthy images and 92 unhealthy images to meet the 80/20 split.","metadata":{}},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential([\n    Input(shape=(128, 128, 3)),  # Define the input shape here\n    Conv2D(32, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:34.592710Z","iopub.execute_input":"2024-11-02T21:00:34.593084Z","iopub.status.idle":"2024-11-02T21:00:34.769357Z","shell.execute_reply.started":"2024-11-02T21:00:34.593045Z","shell.execute_reply":"2024-11-02T21:00:34.768189Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:34.770918Z","iopub.execute_input":"2024-11-02T21:00:34.771405Z","iopub.status.idle":"2024-11-02T21:00:34.789168Z","shell.execute_reply.started":"2024-11-02T21:00:34.771351Z","shell.execute_reply":"2024-11-02T21:00:34.787976Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(len(X_train), len(y_train))  # Ensure both are equal\nprint(len(X_val), len(y_val))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:34.792365Z","iopub.execute_input":"2024-11-02T21:00:34.792783Z","iopub.status.idle":"2024-11-02T21:00:34.799319Z","shell.execute_reply.started":"2024-11-02T21:00:34.792742Z","shell.execute_reply":"2024-11-02T21:00:34.798156Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"1080 1080\n540 540\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Train the model using the data generator for training and the validation set directly\nhistory = model.fit(\n    data_gen.flow(X_train, y_train, batch_size=32),\n    validation_data=(X_val, y_val),\n    epochs=20\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:00:34.800793Z","iopub.execute_input":"2024-11-02T21:00:34.801250Z","iopub.status.idle":"2024-11-02T21:06:43.741391Z","shell.execute_reply.started":"2024-11-02T21:00:34.801199Z","shell.execute_reply":"2024-11-02T21:06:43.740059Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 534ms/step - accuracy: 0.5325 - loss: 1.0087 - val_accuracy: 0.7556 - val_loss: 0.4845\nEpoch 2/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 484ms/step - accuracy: 0.6122 - loss: 0.6211 - val_accuracy: 0.7241 - val_loss: 0.5191\nEpoch 3/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 501ms/step - accuracy: 0.6934 - loss: 0.5930 - val_accuracy: 0.8056 - val_loss: 0.4299\nEpoch 4/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 483ms/step - accuracy: 0.7594 - loss: 0.5403 - val_accuracy: 0.8167 - val_loss: 0.4025\nEpoch 5/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 488ms/step - accuracy: 0.7626 - loss: 0.5203 - val_accuracy: 0.8296 - val_loss: 0.3732\nEpoch 6/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 500ms/step - accuracy: 0.7688 - loss: 0.4756 - val_accuracy: 0.7667 - val_loss: 0.4326\nEpoch 7/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 479ms/step - accuracy: 0.7689 - loss: 0.4775 - val_accuracy: 0.7685 - val_loss: 0.4614\nEpoch 8/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 500ms/step - accuracy: 0.7922 - loss: 0.4589 - val_accuracy: 0.8333 - val_loss: 0.3712\nEpoch 9/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 479ms/step - accuracy: 0.7742 - loss: 0.4612 - val_accuracy: 0.8778 - val_loss: 0.3102\nEpoch 10/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 500ms/step - accuracy: 0.8033 - loss: 0.4433 - val_accuracy: 0.8370 - val_loss: 0.3749\nEpoch 11/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 486ms/step - accuracy: 0.8255 - loss: 0.4179 - val_accuracy: 0.8185 - val_loss: 0.4281\nEpoch 12/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 499ms/step - accuracy: 0.8323 - loss: 0.3548 - val_accuracy: 0.8444 - val_loss: 0.3512\nEpoch 13/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 501ms/step - accuracy: 0.8474 - loss: 0.3827 - val_accuracy: 0.7278 - val_loss: 0.5394\nEpoch 14/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 486ms/step - accuracy: 0.8422 - loss: 0.3448 - val_accuracy: 0.7685 - val_loss: 0.4823\nEpoch 15/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 499ms/step - accuracy: 0.8562 - loss: 0.3530 - val_accuracy: 0.8500 - val_loss: 0.3656\nEpoch 16/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 484ms/step - accuracy: 0.8675 - loss: 0.2927 - val_accuracy: 0.8222 - val_loss: 0.3768\nEpoch 17/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 505ms/step - accuracy: 0.8462 - loss: 0.3320 - val_accuracy: 0.8815 - val_loss: 0.2835\nEpoch 18/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 488ms/step - accuracy: 0.8900 - loss: 0.2759 - val_accuracy: 0.8315 - val_loss: 0.3678\nEpoch 19/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 501ms/step - accuracy: 0.8635 - loss: 0.2976 - val_accuracy: 0.8278 - val_loss: 0.4059\nEpoch 20/20\n\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 482ms/step - accuracy: 0.8446 - loss: 0.3298 - val_accuracy: 0.8648 - val_loss: 0.3145\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Predict on the test set\ny_pred = model.predict(X_test)\ny_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary predictions\n\n# Generate classification report and confusion matrix\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T21:06:43.742944Z","iopub.execute_input":"2024-11-02T21:06:43.743343Z","iopub.status.idle":"2024-11-02T21:06:44.659592Z","shell.execute_reply.started":"2024-11-02T21:06:43.743296Z","shell.execute_reply":"2024-11-02T21:06:44.658462Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step\n              precision    recall  f1-score   support\n\n           0       0.97      0.83      0.89       144\n           1       0.56      0.89      0.69        36\n\n    accuracy                           0.84       180\n   macro avg       0.76      0.86      0.79       180\nweighted avg       0.89      0.84      0.85       180\n\n[[119  25]\n [  4  32]]\n","output_type":"stream"}]}]}